{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import import_ipynb\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#custom Dataset class \n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data, target, transform=None):\n",
    "        self.data = torch.from_numpy(data).float()\n",
    "        self.target = torch.from_numpy(target).long()\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        x = self.data[index]\n",
    "        y = self.target[index]\n",
    "        \n",
    "        if self.transform:\n",
    "            x = self.transform(x)\n",
    "            \n",
    "        return x, y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "numpy_data = np.load('Images/final/final_images.npy')\n",
    "#reshape the data to be consistent with most workflows, ie (1, 512, 512) instead of (512, 512, 1)\n",
    "numpy_data = numpy_data.reshape(numpy_data.shape[0], 1, numpy_data.shape[1], numpy_data.shape[2]) \n",
    "numpy_target = np.load('Images/final/final_labels.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a mediocre train test split\n",
    "#using 75 points in training set, 25 in test set for toy example\n",
    "sample = random.sample(range(800), 800)\n",
    "train_sample = sample[:75]\n",
    "test_sample = sample[75:100]\n",
    "train_data = numpy_data[train_sample]\n",
    "train_target = numpy_target[train_sample]\n",
    "test_data = numpy_data[test_sample]\n",
    "test_target = numpy_data[test_sample]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = MyDataset(train_data, train_target)\n",
    "test_set = MyDataset(test_data, test_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training\n",
    "n_training_samples = 75\n",
    "train_sampler = SubsetRandomSampler(np.arange(n_training_samples, dtype=np.int64))\n",
    "\n",
    "#Test\n",
    "n_test_samples = 25\n",
    "test_sampler = SubsetRandomSampler(np.arange(n_test_samples, dtype=np.int64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(torch.nn.Module):\n",
    "    \n",
    "    #Our batch shape for input x is (1, 512, 512)\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        \n",
    "        #Input channels = 1, output channels = 16\n",
    "        self.conv1 = torch.nn.Conv2d(1, 16, kernel_size=3, stride=2, padding=1)\n",
    "        self.pool1 = torch.nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.pool2 = torch.nn.MaxPool2d(kernel_size = 3, stride = 2, padding = 1)\n",
    "        self.pool3 = torch.nn.MaxPool2d(kernel_size = 3, stride = 2, padding = 1)\n",
    "        self.pool4 = torch.nn.MaxPool2d(kernel_size = 3, stride = 2, padding = 1)\n",
    "        \n",
    "        #4608 input features, 64 output features (see sizing flow below)\n",
    "        self.fc1 = torch.nn.Linear(16 * 16 * 16, 64)\n",
    "        \n",
    "        #64 input features, 10 output features for our 2 defined classes\n",
    "        self.fc2 = torch.nn.Linear(64, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        #Computes the activation of the first convolution\n",
    "        #Size changes from (1, 512, 512) to (16, 512, 512)\n",
    "        x = F.relu(self.conv1(x))\n",
    "        \n",
    "        #Size changes from (16, 512, 512) to (18, 256, 256)\n",
    "        x = self.pool1(x)\n",
    "        #Size changes from (16, 256, 256) to (16, 128, 128)\n",
    "        x = self.pool2(x)\n",
    "        #Size changes from (16, 128, 128) to (16, 64, 64)\n",
    "        x = self.pool3(x)        \n",
    "        #Size changes from (16, 64, 64) to (16, 32, 32)\n",
    "        x = self.pool4(x)   \n",
    "        #Size changes from (16, 64, 64) to (16, 16, 16)\n",
    "        \n",
    "        #Reshape data to input to the input layer of the neural net\n",
    "        #Size changes from (16, 16, 16) to (1, 4096)\n",
    "        #Recall that the -1 infers this dimension from the other given dimension\n",
    "        x = x.view(-1, 4096)\n",
    "        \n",
    "        #Computes the activation of the first fully connected layer\n",
    "        #Size changes from (1, 4608) to (1, 64)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        \n",
    "        #Computes the second fully connected layer (activation applied later)\n",
    "        #Size changes from (1, 64) to (1, 2)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def outputSize(in_size, kernel_size, stride, padding):\n",
    "    \n",
    "    #helpful when filling in pooling layers\n",
    "    \n",
    "    output = int((in_size - kernel_size + 2*(padding)) / stride) + 1\n",
    "    return(output)\n",
    "outputSize(in_size = 32, kernel_size = 3, stride = 2, padding = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_loader(batch_size):\n",
    "    train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size,\n",
    "                                           sampler=train_sampler, num_workers=2)\n",
    "    return(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=1, sampler=test_sampler, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createLossAndOptimizer(net, learning_rate=0.001):\n",
    "    \n",
    "    #Loss function\n",
    "    loss = torch.nn.CrossEntropyLoss()\n",
    "    \n",
    "    #Optimizer\n",
    "    optimizer = optim.Adam(net.parameters(), lr=learning_rate)\n",
    "    \n",
    "    return(loss, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainNet(net, batch_size, n_epochs, learning_rate):\n",
    "    \n",
    "    #Print all of the hyperparameters of the training iteration:\n",
    "    print(\"===== HYPERPARAMETERS =====\")\n",
    "    print(\"batch_size=\", batch_size)\n",
    "    print(\"epochs=\", n_epochs)\n",
    "    print(\"learning_rate=\", learning_rate)\n",
    "    print(\"=\" * 30)\n",
    "    \n",
    "    #Get training data\n",
    "    train_loader = get_train_loader(batch_size)\n",
    "    n_batches = len(train_loader)\n",
    "    \n",
    "    #Create our loss and optimizer functions\n",
    "    loss, optimizer = createLossAndOptimizer(net, learning_rate)\n",
    "    \n",
    "    #Time for printing\n",
    "    training_start_time = time.time()\n",
    "    \n",
    "    #Loop for n_epochs\n",
    "    for epoch in range(n_epochs):\n",
    "        \n",
    "        running_loss = 0.0\n",
    "        print_every = n_batches // 10\n",
    "        start_time = time.time()\n",
    "        total_train_loss = 0\n",
    "        \n",
    "        for i, data in enumerate(train_loader, 0):\n",
    "            \n",
    "            #Get inputs\n",
    "            inputs, labels = data\n",
    "            \n",
    "            #Wrap them in a Variable object\n",
    "            inputs, labels = Variable(inputs), Variable(labels)\n",
    "            \n",
    "            #Set the parameter gradients to zero\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            #Forward pass, backward pass, optimize\n",
    "            outputs = net(inputs)\n",
    "            loss_size = loss(outputs, labels)\n",
    "            loss_size.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            #Print statistics\n",
    "            running_loss += loss_size.item()\n",
    "            total_train_loss += loss_size.item()\n",
    "            \n",
    "            #Print every 10th batch of an epoch\n",
    "            if (i + 1) % (print_every + 1) == 0:\n",
    "                print(\"Epoch {}, {:d}% \\t train_loss: {:.2f} took: {:.2f}s\".format(\n",
    "                        epoch+1, int(100 * (i+1) / n_batches), running_loss / print_every, time.time() - start_time))\n",
    "                #Reset running loss and time\n",
    "                running_loss = 0.0\n",
    "                start_time = time.time()\n",
    "\n",
    "    print(\"Training finished, took {:.2f}s\".format(time.time() - training_start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== HYPERPARAMETERS =====\n",
      "batch_size= 1\n",
      "epochs= 10\n",
      "learning_rate= 0.001\n",
      "==============================\n",
      "Epoch 1, 10% \t train_loss: 2.21 took: 0.30s\n",
      "Epoch 1, 21% \t train_loss: 0.65 took: 0.19s\n",
      "Epoch 1, 32% \t train_loss: 0.72 took: 0.20s\n",
      "Epoch 1, 42% \t train_loss: 0.88 took: 0.19s\n",
      "Epoch 1, 53% \t train_loss: 0.78 took: 0.20s\n",
      "Epoch 1, 64% \t train_loss: 1.31 took: 0.20s\n",
      "Epoch 1, 74% \t train_loss: 0.92 took: 0.20s\n",
      "Epoch 1, 85% \t train_loss: 1.27 took: 0.19s\n",
      "Epoch 1, 96% \t train_loss: 0.53 took: 0.20s\n",
      "Epoch 2, 10% \t train_loss: 0.78 took: 0.25s\n",
      "Epoch 2, 21% \t train_loss: 0.66 took: 0.19s\n",
      "Epoch 2, 32% \t train_loss: 0.55 took: 0.20s\n",
      "Epoch 2, 42% \t train_loss: 0.57 took: 0.19s\n",
      "Epoch 2, 53% \t train_loss: 0.61 took: 0.19s\n",
      "Epoch 2, 64% \t train_loss: 0.51 took: 0.19s\n",
      "Epoch 2, 74% \t train_loss: 1.06 took: 0.19s\n",
      "Epoch 2, 85% \t train_loss: 0.70 took: 0.19s\n",
      "Epoch 2, 96% \t train_loss: 0.79 took: 0.19s\n",
      "Epoch 3, 10% \t train_loss: 0.59 took: 0.25s\n",
      "Epoch 3, 21% \t train_loss: 0.52 took: 0.19s\n",
      "Epoch 3, 32% \t train_loss: 0.31 took: 0.20s\n",
      "Epoch 3, 42% \t train_loss: 1.08 took: 0.19s\n",
      "Epoch 3, 53% \t train_loss: 0.80 took: 0.19s\n",
      "Epoch 3, 64% \t train_loss: 0.53 took: 0.19s\n",
      "Epoch 3, 74% \t train_loss: 1.22 took: 0.19s\n",
      "Epoch 3, 85% \t train_loss: 0.73 took: 0.19s\n",
      "Epoch 3, 96% \t train_loss: 0.96 took: 0.20s\n",
      "Epoch 4, 10% \t train_loss: 0.77 took: 0.25s\n",
      "Epoch 4, 21% \t train_loss: 0.43 took: 0.19s\n",
      "Epoch 4, 32% \t train_loss: 0.47 took: 0.20s\n",
      "Epoch 4, 42% \t train_loss: 0.31 took: 0.19s\n",
      "Epoch 4, 53% \t train_loss: 0.47 took: 0.19s\n",
      "Epoch 4, 64% \t train_loss: 0.79 took: 0.19s\n",
      "Epoch 4, 74% \t train_loss: 0.77 took: 0.20s\n",
      "Epoch 4, 85% \t train_loss: 0.48 took: 0.20s\n",
      "Epoch 4, 96% \t train_loss: 0.69 took: 0.20s\n",
      "Epoch 5, 10% \t train_loss: 0.71 took: 0.26s\n",
      "Epoch 5, 21% \t train_loss: 0.45 took: 0.21s\n",
      "Epoch 5, 32% \t train_loss: 0.54 took: 0.20s\n",
      "Epoch 5, 42% \t train_loss: 0.68 took: 0.20s\n",
      "Epoch 5, 53% \t train_loss: 0.26 took: 0.19s\n",
      "Epoch 5, 64% \t train_loss: 0.64 took: 0.20s\n",
      "Epoch 5, 74% \t train_loss: 0.63 took: 0.19s\n",
      "Epoch 5, 85% \t train_loss: 0.31 took: 0.19s\n",
      "Epoch 5, 96% \t train_loss: 0.88 took: 0.19s\n",
      "Epoch 6, 10% \t train_loss: 0.73 took: 0.25s\n",
      "Epoch 6, 21% \t train_loss: 0.59 took: 0.19s\n",
      "Epoch 6, 32% \t train_loss: 0.42 took: 0.20s\n",
      "Epoch 6, 42% \t train_loss: 0.34 took: 0.19s\n",
      "Epoch 6, 53% \t train_loss: 0.43 took: 0.20s\n",
      "Epoch 6, 64% \t train_loss: 0.36 took: 0.19s\n",
      "Epoch 6, 74% \t train_loss: 0.38 took: 0.20s\n",
      "Epoch 6, 85% \t train_loss: 0.33 took: 0.20s\n",
      "Epoch 6, 96% \t train_loss: 0.80 took: 0.20s\n",
      "Epoch 7, 10% \t train_loss: 0.31 took: 0.27s\n",
      "Epoch 7, 21% \t train_loss: 0.51 took: 0.20s\n",
      "Epoch 7, 32% \t train_loss: 0.70 took: 0.20s\n",
      "Epoch 7, 42% \t train_loss: 0.72 took: 0.21s\n",
      "Epoch 7, 53% \t train_loss: 0.21 took: 0.19s\n",
      "Epoch 7, 64% \t train_loss: 0.56 took: 0.20s\n",
      "Epoch 7, 74% \t train_loss: 0.21 took: 0.19s\n",
      "Epoch 7, 85% \t train_loss: 0.63 took: 0.19s\n",
      "Epoch 7, 96% \t train_loss: 0.55 took: 0.19s\n",
      "Epoch 8, 10% \t train_loss: 0.28 took: 0.25s\n",
      "Epoch 8, 21% \t train_loss: 0.55 took: 0.19s\n",
      "Epoch 8, 32% \t train_loss: 0.19 took: 0.19s\n",
      "Epoch 8, 42% \t train_loss: 0.79 took: 0.19s\n",
      "Epoch 8, 53% \t train_loss: 0.34 took: 0.19s\n",
      "Epoch 8, 64% \t train_loss: 0.57 took: 0.19s\n",
      "Epoch 8, 74% \t train_loss: 0.72 took: 0.20s\n",
      "Epoch 8, 85% \t train_loss: 0.40 took: 0.19s\n",
      "Epoch 8, 96% \t train_loss: 0.57 took: 0.20s\n",
      "Epoch 9, 10% \t train_loss: 0.59 took: 0.26s\n",
      "Epoch 9, 21% \t train_loss: 0.36 took: 0.20s\n",
      "Epoch 9, 32% \t train_loss: 0.81 took: 0.20s\n",
      "Epoch 9, 42% \t train_loss: 0.68 took: 0.20s\n",
      "Epoch 9, 53% \t train_loss: 0.23 took: 0.20s\n",
      "Epoch 9, 64% \t train_loss: 0.20 took: 0.21s\n",
      "Epoch 9, 74% \t train_loss: 0.30 took: 0.20s\n",
      "Epoch 9, 85% \t train_loss: 0.54 took: 0.20s\n",
      "Epoch 9, 96% \t train_loss: 0.36 took: 0.20s\n",
      "Epoch 10, 10% \t train_loss: 0.31 took: 0.25s\n",
      "Epoch 10, 21% \t train_loss: 0.29 took: 0.19s\n",
      "Epoch 10, 32% \t train_loss: 0.57 took: 0.20s\n",
      "Epoch 10, 42% \t train_loss: 0.45 took: 0.20s\n",
      "Epoch 10, 53% \t train_loss: 0.03 took: 0.21s\n",
      "Epoch 10, 64% \t train_loss: 0.44 took: 0.20s\n",
      "Epoch 10, 74% \t train_loss: 0.68 took: 0.21s\n",
      "Epoch 10, 85% \t train_loss: 0.43 took: 0.21s\n",
      "Epoch 10, 96% \t train_loss: 0.21 took: 0.21s\n",
      "Training finished, took 19.11s\n"
     ]
    }
   ],
   "source": [
    "#create CNN instance\n",
    "CNN = CNN()\n",
    "#train it\n",
    "trainNet(CNN, batch_size=1, n_epochs=10, learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
